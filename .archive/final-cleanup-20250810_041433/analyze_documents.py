#!/usr/bin/env python3
"""
Clauder ë¬¸ì„œ ë¶„ì„ ë° ë§ˆì´ê·¸ë ˆì´ì…˜ ë§¤í•‘ ìƒì„±
"""

import os
import re
import json
import hashlib
from collections import defaultdict
from datetime import datetime
from pathlib import Path

class DocumentAnalyzer:
    def __init__(self, root_dir='.'):
        self.root_dir = root_dir
        self.documents = []
        self.duplicates = defaultdict(list)
        self.similar = defaultdict(list)
        self.categories = defaultdict(list)
        
    def analyze(self):
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸ“Š ë¬¸ì„œ ë¶„ì„ ì‹œì‘...")
        self.scan_documents()
        self.detect_duplicates()
        self.categorize_documents()
        self.find_similar_content()
        self.generate_migration_map()
        
    def scan_documents(self):
        """ëª¨ë“  ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìŠ¤ìº”"""
        exclude_dirs = {'.git', 'node_modules', '.archive', '.vscode'}
        
        for root, dirs, files in os.walk(self.root_dir):
            # ì œì™¸ ë””ë ‰í† ë¦¬ ê±´ë„ˆë›°ê¸°
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                if file.endswith('.md'):
                    filepath = os.path.join(root, file)
                    doc_info = self.analyze_document(filepath)
                    if doc_info:
                        self.documents.append(doc_info)
        
        print(f"âœ“ {len(self.documents)}ê°œ ë¬¸ì„œ ìŠ¤ìº” ì™„ë£Œ")
        
    def analyze_document(self, filepath):
        """ê°œë³„ ë¬¸ì„œ ë¶„ì„"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # ë¹ˆ íŒŒì¼ ê±´ë„ˆë›°ê¸°
            if not content.strip():
                return None
                
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self.extract_metadata(content)
            
            # ë‚´ìš© í•´ì‹œ ìƒì„±
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            # ì°¸ì¡° ì¶”ì¶œ
            references = re.findall(r'@[./\w-]+\.md', content)
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¸¡
            category = self.guess_category(filepath, content)
            
            return {
                'path': filepath,
                'relative_path': os.path.relpath(filepath, self.root_dir),
                'size': len(content),
                'lines': content.count('\n'),
                'hash': content_hash,
                'metadata': metadata,
                'references': references,
                'category': category,
                'title': self.extract_title(content),
                'has_code': '```' in content,
                'has_yaml': content.startswith('---'),
            }
            
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {filepath} - {e}")
            return None
            
    def extract_metadata(self, content):
        """YAML front matter ì¶”ì¶œ"""
        metadata = {}
        if content.startswith('---'):
            try:
                yaml_match = re.search(r'^---\n(.*?)\n---', content, re.DOTALL)
                if yaml_match:
                    yaml_content = yaml_match.group(1)
                    for line in yaml_content.split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            metadata[key.strip()] = value.strip()
            except:
                pass
        return metadata
        
    def extract_title(self, content):
        """ë¬¸ì„œ ì œëª© ì¶”ì¶œ"""
        title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
        if title_match:
            return title_match.group(1)
        return None
        
    def guess_category(self, filepath, content):
        """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¸¡"""
        path_lower = filepath.lower()
        
        # ê²½ë¡œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬
        if 'principle' in path_lower:
            return 'principles'
        elif 'workflow' in path_lower:
            return 'workflow'
        elif 'learn' in path_lower or 'analysis' in path_lower:
            return 'learning'
        elif 'tool' in path_lower or 'hook' in path_lower or 'validator' in path_lower:
            return 'tools'
        elif 'template' in path_lower:
            return 'templates'
        elif 'doc' in path_lower or 'guide' in path_lower:
            return 'documentation'
        elif 'test' in path_lower or 'example' in path_lower:
            return 'examples'
        
        # ë‚´ìš© ê¸°ë°˜ ì¹´í…Œê³ ë¦¬
        content_lower = content.lower()
        if 'principle' in content_lower or 'ì›ì¹™' in content_lower:
            return 'principles'
        elif 'workflow' in content_lower or 'ì›Œí¬í”Œë¡œ' in content_lower:
            return 'workflow'
        elif 'learn' in content_lower or 'í•™ìŠµ' in content_lower:
            return 'learning'
        
        return 'misc'
        
    def detect_duplicates(self):
        """ì¤‘ë³µ ë¬¸ì„œ ê°ì§€"""
        hash_map = defaultdict(list)
        
        for doc in self.documents:
            hash_map[doc['hash']].append(doc['path'])
            
        for hash_val, paths in hash_map.items():
            if len(paths) > 1:
                self.duplicates[hash_val] = paths
                
        print(f"âœ“ {len(self.duplicates)}ê°œ ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬")
        
    def categorize_documents(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜"""
        for doc in self.documents:
            self.categories[doc['category']].append(doc['relative_path'])
            
        print("âœ“ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜:")
        for category, docs in sorted(self.categories.items()):
            print(f"  - {category}: {len(docs)}ê°œ")
            
    def find_similar_content(self):
        """ìœ ì‚¬ ë‚´ìš© ì°¾ê¸° (ì œëª© ê¸°ë°˜)"""
        title_map = defaultdict(list)
        
        for doc in self.documents:
            if doc['title']:
                # ì œëª© ì •ê·œí™”
                normalized = re.sub(r'[^a-zA-Z0-9ê°€-í£]', '', doc['title'].lower())
                title_map[normalized].append(doc['path'])
                
        for title, paths in title_map.items():
            if len(paths) > 1:
                self.similar[title] = paths
                
        print(f"âœ“ {len(self.similar)}ê°œ ìœ ì‚¬ ë¬¸ì„œ ê·¸ë£¹ ë°œê²¬")
        
    def generate_migration_map(self):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë§µ ìƒì„±"""
        migration_map = {
            'timestamp': datetime.now().isoformat(),
            'total_documents': len(self.documents),
            'categories': {},
            'duplicates': [],
            'similar': [],
            'migrations': []
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²½ë¡œ
        category_mapping = {
            'principles': '.principles/',
            'workflow': '.workflow/',
            'learning': '.learning/',
            'tools': '.tools/',
            'templates': '.templates/',
            'documentation': '.docs/',
            'examples': '.examples/',
            'misc': '.archive/misc/'
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
        for category, docs in self.categories.items():
            migration_map['categories'][category] = {
                'count': len(docs),
                'target': category_mapping.get(category, '.archive/'),
                'documents': docs
            }
            
        # ì¤‘ë³µ ë¬¸ì„œ ì²˜ë¦¬
        for hash_val, paths in self.duplicates.items():
            migration_map['duplicates'].append({
                'hash': hash_val,
                'paths': paths,
                'action': 'merge',
                'keep': paths[0],  # ì²« ë²ˆì§¸ ê²ƒ ìœ ì§€
                'remove': paths[1:]
            })
            
        # ìœ ì‚¬ ë¬¸ì„œ ì²˜ë¦¬
        for title, paths in self.similar.items():
            migration_map['similar'].append({
                'title': title,
                'paths': paths,
                'action': 'review',  # ìˆ˜ë™ ê²€í†  í•„ìš”
            })
            
        # ê°œë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš
        for doc in self.documents:
            old_path = doc['relative_path']
            new_dir = category_mapping.get(doc['category'], '.archive/')
            
            # ë””ë ‰í† ë¦¬ êµ¬ì¡° ê°œì„ 
            if 'clauder-dev' in old_path:
                new_dir += 'development/'
            elif '.claude/' in old_path:
                new_dir += 'usage/'
            elif '.base-principles/' in old_path:
                new_dir += 'base/'
                
            new_path = new_dir + os.path.basename(old_path)
            
            migration_map['migrations'].append({
                'old': old_path,
                'new': new_path,
                'category': doc['category'],
                'size': doc['size'],
                'has_references': len(doc['references']) > 0
            })
            
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open('migration_map.json', 'w', encoding='utf-8') as f:
            json.dump(migration_map, f, indent=2, ensure_ascii=False)
            
        print("\nğŸ“‹ ë§ˆì´ê·¸ë ˆì´ì…˜ ë§µ ìƒì„± ì™„ë£Œ: migration_map.json")
        
        # ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ìš”ì•½:")
        print(f"- ì´ ë¬¸ì„œ: {len(self.documents)}ê°œ")
        print(f"- ì¤‘ë³µ ì œê±° ê°€ëŠ¥: {sum(len(p)-1 for p in self.duplicates.values())}ê°œ")
        print(f"- ë³‘í•© ê²€í†  í•„ìš”: {len(self.similar)}ê°œ ê·¸ë£¹")
        print(f"- ì¹´í…Œê³ ë¦¬: {len(self.categories)}ê°œ")
        
        # ì¶”ì²œ ì‚¬í•­
        print("\nğŸ’¡ ì¶”ì²œ ì‚¬í•­:")
        if self.duplicates:
            print("- ì¤‘ë³µ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        if self.similar:
            print("- ìœ ì‚¬ ë¬¸ì„œëŠ” ìˆ˜ë™ ê²€í†  í›„ ë³‘í•©ì„ ê³ ë ¤í•˜ì„¸ìš”")
        if 'misc' in self.categories and len(self.categories['misc']) > 10:
            print("- 'misc' ì¹´í…Œê³ ë¦¬ ë¬¸ì„œë“¤ì„ ì¬ë¶„ë¥˜í•˜ì„¸ìš”")
            
        return migration_map

if __name__ == '__main__':
    analyzer = DocumentAnalyzer()
    analyzer.analyze()
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print("ë‹¤ìŒ ë‹¨ê³„: ./execute_migration.sh migration_map.json")